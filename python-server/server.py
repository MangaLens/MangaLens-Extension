from flask import Flask, request, jsonify
from flask_cors import CORS
import base64
from io import BytesIO
from PIL import Image
import torch
import gc
from transformers import AutoModelForCausalLM, AutoProcessor
from surya.detection import DetectionPredictor
from utils import merge_overlapping_boxes, image_enhance

app = Flask(__name__)
CORS(app)

print('Initializing models...')

# GPU ì„¤ì •
device = "mps" if torch.backends.mps.is_available() else "cpu"

# â­ ëª¨ë¸ì„ ì „ì—­ ë³€ìˆ˜ë¡œ í•œ ë²ˆë§Œ ë¡œë”© (ì„œë²„ ì‹œì‘ ì‹œ)
det_predictor = DetectionPredictor()

model_path = "weights/DotsOCR"

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    local_files_only=True
).to(device)

processor = AutoProcessor.from_pretrained(
    model_path,
    trust_remote_code=True,
    local_files_only=True
)

# ë²ˆì—­ ëª¨ë¸ì€ OpenAI API ì‚¬ìš©
translator = None

def get_translator():
    """OpenAI API ì—°ê²° í™•ì¸ (LM Studio ë˜ëŠ” ì‹¤ì œ OpenAI)"""
    global translator
    if translator is None:
        print('Connecting to OpenAI API...')
        try:
            from openai import OpenAI

            # LM Studioì˜ ë¡œì»¬ ì„œë²„ ì‚¬ìš©
            client = OpenAI(
                base_url="http://localhost:1234/v1",
                api_key="lm-studio"  # LM StudioëŠ” API í‚¤ê°€ í•„ìš” ì—†ì§€ë§Œ í˜•ì‹ìƒ í•„ìš”
            )

            # ì—°ê²° í…ŒìŠ¤íŠ¸
            test_response = client.chat.completions.create(
                model="local-model",  # LM StudioëŠ” ëª¨ë¸ëª…ì´ ì¤‘ìš”í•˜ì§€ ì•ŠìŒ
                messages=[{"role": "user", "content": "test"}],
                max_tokens=1
            )

            translator = {
                'type': 'openai',
                'client': client
            }
            print('âœ“ Connected to OpenAI API (LM Studio)')

        except Exception as e:
            print(f'âš ï¸  OpenAI API connection failed: {e}')
            print('   Make sure LM Studio is running with a model loaded on port 1234')
            return None
    return translator

print('âœ“ Models ready')

PROMPT = """Extract the text content from this image."""

def translate_text(text, target_lang="Korean"):
    """í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•˜ëŠ” í•¨ìˆ˜ - OpenAI API ì‚¬ìš©"""
    if not text or not text.strip():
        return text

    trans = get_translator()
    if trans is None:
        print('âš ï¸  Translation skipped - API not available')
        return text

    try:
        client = trans['client']

        response = client.chat.completions.create(
            model="local-model",  # LM StudioëŠ” ë¡œë“œëœ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì‚¬ìš©
            messages=[
                {
                    "role": "user",
                    "content": f"Translate the following text into {target_lang}.\nText: {text}\n{target_lang}:"
                }
            ],
            temperature=0.1,
            max_tokens=256
        )

        translation = response.choices[0].message.content.strip()
        return translation

    except Exception as e:
        print(f'âš ï¸  Translation error: {e}')
        import traceback
        traceback.print_exc()
        return text

@app.route('/ocr', methods=['POST'])
def ocr_endpoint():
    print('=' * 50)

    try:
        data = request.get_json()
        image_data_url = data.get('image')

        if not image_data_url:
            return jsonify({'error': 'No image'}), 400

        if ',' in image_data_url:
            image_data_url = image_data_url.split(',', 1)[1]

        image_bytes = base64.b64decode(image_data_url)
        image = Image.open(BytesIO(image_bytes))

        if image.mode != 'RGB':
            image = image.convert('RGB')

        image = image_enhance(image)

        print(f'Processing: {image.size}')

        # 1ë‹¨ê³„: Suryaë¡œ í…ìŠ¤íŠ¸ ë²„ë¸” ê°ì§€
        print('ğŸ” Detecting text bubbles with Surya...')
        predictions = det_predictor([image])
        text_bboxes = merge_overlapping_boxes(predictions[0].bboxes, 10)

        # ì‘ì€ ë²„ë¸” í•„í„°ë§
        MIN_WIDTH = 30
        MIN_HEIGHT = 30
        MIN_AREA = 900

        filtered_bboxes = []
        for bbox in text_bboxes:
            x1, y1, x2, y2 = bbox
            width = x2 - x1
            height = y2 - y1
            area = width * height

            if width >= MIN_WIDTH and height >= MIN_HEIGHT and area >= MIN_AREA:
                filtered_bboxes.append(bbox)

        text_bboxes = filtered_bboxes
        print(f'âœ“ Detected {len(text_bboxes)} text bubbles')

        # 2ë‹¨ê³„: ê° ë²„ë¸”ì— ëŒ€í•´ OCR ìˆ˜í–‰
        text_blocks = []

        for idx, bbox in enumerate(text_bboxes):
            x1, y1, x2, y2 = map(int, bbox)
            cropped = image.crop((x1, y1, x2, y2))

            print(f'ğŸ”„ OCR {idx + 1}/{len(text_bboxes)} at ({x1}, {y1}, {x2}, {y2})')

            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": cropped},
                        {"type": "text", "text": PROMPT}
                    ]
                }
            ]

            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = processor(text=[text], images=[cropped], padding=True, return_tensors="pt")

            inputs = {
                k: v.to(device).to(torch.float16) if isinstance(v, torch.Tensor) and v.dtype in [torch.float32, torch.bfloat16]
                else v.to(device) if isinstance(v, torch.Tensor)
                else v
                for k, v in inputs.items()
            }

            with torch.no_grad():
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=64,
                    do_sample=False,
                    num_beams=1,
                )

            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)
            ]

            output_text = processor.batch_decode(
                generated_ids_trimmed,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]

            print(f'   Text: {output_text}')

            # ë²ˆì—­ ìˆ˜í–‰
            print(f'ğŸŒ Translating...')
            translated_text = translate_text(output_text, target_lang="Korean")
            print(f'   Translated: {translated_text}')

            # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë¬¸ ì‚¬ìš©
            display_text = translated_text if translated_text and translated_text != output_text else output_text

            # ë²ˆì—­ëœ í…ìŠ¤íŠ¸ë¥¼ í‘œì‹œ (ì›ë³¸ì€ ë³´ê´€ë§Œ)
            text_blocks.append({
                'text': display_text,  # í™”ë©´ì— í‘œì‹œë  í…ìŠ¤íŠ¸ (ë²ˆì—­ë³¸ ë˜ëŠ” ì›ë¬¸)
                'original_text': output_text,  # OCR ì›ë³¸
                'translated_text': translated_text if translated_text != output_text else None,  # ë²ˆì—­ë³¸ (ì„±ê³µ ì‹œë§Œ)
                'bbox': {
                    'x0': x1,
                    'y0': y1,
                    'x1': x2,
                    'y1': y2
                },
                'type': 'text_bubble',
                'style': 'normal'
            })

            # â­ í…ì„œë§Œ ì‚­ì œ (ëª¨ë¸ì€ ìœ ì§€)
            del inputs, generated_ids, generated_ids_trimmed

        # â­ ìš”ì²­ ì²˜ë¦¬ í›„ í•œ ë²ˆë§Œ ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()
        gc.collect()

        full_text = '\n'.join([b['text'] for b in text_blocks])  # ì´ì œ ë²ˆì—­ëœ í…ìŠ¤íŠ¸

        print(f'âœ“ Extracted and translated {len(text_blocks)} text blocks')
        print('=' * 50)

        return jsonify({
            'text': full_text,  # ë²ˆì—­ëœ í…ìŠ¤íŠ¸
            'text_blocks': text_blocks,  # text í•„ë“œì— ë²ˆì—­ë³¸ì´ ë“¤ì–´ìˆìŒ
            'success': True,
            'bubbles_count': len(text_blocks)
        })

    except Exception as e:
        print(f'âŒ Error: {e}')
        import traceback
        traceback.print_exc()
        print('=' * 50)
        return jsonify({'error': str(e)}), 500

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'ok'})

if __name__ == '__main__':
    print('Starting server on http://127.0.0.1:5000')
    app.run(host='127.0.0.1', port=5000, debug=True)